{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b635fc8c",
   "metadata": {},
   "source": [
    "# ‚ö° Aula 3 ‚Äì GPUs em Python e Aplica√ß√µes em Engenharia\n",
    "\n",
    "## Computa√ß√£o de Alto Desempenho em Python para Engenharia Civil\n",
    "\n",
    "**Objetivos desta aula:**\n",
    "- Introduzir paralelismo massivo com GPU\n",
    "- Usar CuPy (NumPy para GPU) e Numba CUDA\n",
    "- Aplicar HPC a simula√ß√µes reais (difus√£o de calor)\n",
    "- Comparar performance CPU vs GPU\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ CPU vs GPU: Filosofias Diferentes\n",
    "\n",
    "**CPU (Central Processing Unit):**\n",
    "- Poucos n√∫cleos complexos (4-64 cores)\n",
    "- Otimizada para lat√™ncia (velocidade individual)\n",
    "- Hierarquia de cache complexa\n",
    "- Ideal para c√≥digo sequencial e ramificado\n",
    "\n",
    "**GPU (Graphics Processing Unit):**\n",
    "- Milhares de n√∫cleos simples (1000+ cores)\n",
    "- Otimizada para throughput (trabalho total)\n",
    "- Arquitetura SIMD (Single Instruction, Multiple Data)\n",
    "- Ideal para paralelismo massivo e dados regulares\n",
    "\n",
    "### üèóÔ∏è Aplica√ß√µes GPU em Engenharia Civil\n",
    "- **Simula√ß√µes CFD** (Computational Fluid Dynamics)\n",
    "- **An√°lise modal** de estruturas complexas\n",
    "- **Processamento de imagens** (inspe√ß√£o, monitoramento)\n",
    "- **Machine Learning** para predi√ß√£o de falhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c41cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import multiprocessing as mp\n",
    "\n",
    "# Tentar importar bibliotecas GPU\n",
    "try:\n",
    "    import cupy as cp\n",
    "    CUPY_AVAILABLE = True\n",
    "    print(f\"‚úÖ CuPy {cp.__version__} dispon√≠vel\")\n",
    "    print(f\"   GPU: {cp.cuda.runtime.getDeviceProperties(0)['name'].decode()}\")\n",
    "    print(f\"   Mem√≥ria: {cp.cuda.runtime.memGetInfo()[1] // (1024**3)} GB\")\n",
    "except ImportError:\n",
    "    CUPY_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  CuPy n√£o dispon√≠vel - instale com: pip install cupy-cuda11x (ou cupy-cuda12x)\")\n",
    "\n",
    "try:\n",
    "    from numba import cuda\n",
    "    import numba\n",
    "    NUMBA_CUDA_AVAILABLE = True\n",
    "    print(f\"‚úÖ Numba CUDA {numba.__version__} dispon√≠vel\")\n",
    "except ImportError:\n",
    "    NUMBA_CUDA_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  Numba CUDA n√£o dispon√≠vel\")\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8' if 'seaborn-v0_8' in plt.style.available else 'default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(f\"nüñ•Ô∏è  Sistema: {mp.cpu_count()} n√∫cleos CPU\")\n",
    "print(f\"üìä NumPy: {np.__version__}\")\n",
    "\n",
    "if not CUPY_AVAILABLE and not NUMBA_CUDA_AVAILABLE:\n",
    "    print(\"n‚ö†Ô∏è  Nota: Esta aula requer GPU NVIDIA e CUDA para funcionalidade completa\")\n",
    "    print(\"   Mas ainda podemos aprender os conceitos e ver simula√ß√µes CPU!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb17455",
   "metadata": {},
   "source": [
    "## 1. Exemplo 10: Soma de Vetores com CuPy\n",
    "\n",
    "CuPy oferece uma interface NumPy para GPUs - mudan√ßa m√≠nima de c√≥digo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1cbf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUPY_AVAILABLE:\n",
    "    print(\"üöÄ Exemplo 10: Soma de Vetores com CuPy\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Teste com diferentes tamanhos de vetores\n",
    "    sizes = [1_000_000, 10_000_000, 100_000_000]\n",
    "    \n",
    "    for N in sizes:\n",
    "        print(f\"nüìä Tamanho: {N:,} elementos\")\n",
    "        \n",
    "        # Criar vetores no CPU\n",
    "        print(\"  Criando dados no CPU...\")\n",
    "        a_cpu = np.random.randn(N).astype(np.float32)\n",
    "        b_cpu = np.random.randn(N).astype(np.float32)\n",
    "        \n",
    "        # NumPy (CPU)\n",
    "        start = time.perf_counter()\n",
    "        c_cpu = a_cpu + b_cpu\n",
    "        time_cpu = time.perf_counter() - start\n",
    "        print(f\"  NumPy (CPU):     {time_cpu:.4f}s\")\n",
    "        \n",
    "        # Transferir para GPU\n",
    "        start_transfer = time.perf_counter()\n",
    "        a_gpu = cp.asarray(a_cpu)\n",
    "        b_gpu = cp.asarray(b_cpu)\n",
    "        time_transfer_to = time.perf_counter() - start_transfer\n",
    "        \n",
    "        # CuPy (GPU) - primeira execu√ß√£o\n",
    "        start = time.perf_counter()\n",
    "        c_gpu = a_gpu + b_gpu\n",
    "        cp.cuda.Stream.null.synchronize()  # Aguardar conclus√£o\n",
    "        time_gpu_first = time.perf_counter() - start\n",
    "        \n",
    "        # CuPy (GPU) - segunda execu√ß√£o (kernels j√° compilados)\n",
    "        start = time.perf_counter()\n",
    "        c_gpu = a_gpu + b_gpu\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        time_gpu = time.perf_counter() - start\n",
    "        \n",
    "        # Transferir resultado de volta\n",
    "        start_transfer = time.perf_counter()\n",
    "        c_gpu_cpu = cp.asnumpy(c_gpu)\n",
    "        time_transfer_back = time.perf_counter() - start_transfer\n",
    "        \n",
    "        # Verificar corre√ß√£o\n",
    "        are_equal = np.allclose(c_cpu, c_gpu_cpu, rtol=1e-5)\n",
    "        \n",
    "        # Speedup considerando apenas computa√ß√£o\n",
    "        speedup_compute = time_cpu / time_gpu\n",
    "        \n",
    "        # Speedup total (incluindo transfer√™ncias)\n",
    "        total_gpu_time = time_transfer_to + time_gpu + time_transfer_back\n",
    "        speedup_total = time_cpu / total_gpu_time\n",
    "        \n",
    "        print(f\"  CuPy (GPU):      {time_gpu:.4f}s\")\n",
    "        print(f\"  Transfer√™ncia:   {time_transfer_to + time_transfer_back:.4f}s\")\n",
    "        print(f\"  Total GPU:       {total_gpu_time:.4f}s\")\n",
    "        print(f\"  Speedup (comp):  {speedup_compute:.2f}x\")\n",
    "        print(f\"  Speedup (total): {speedup_total:.2f}x\")\n",
    "        print(f\"  Precis√£o:        {'‚úì' if are_equal else '‚úó'}\")\n",
    "    \n",
    "    print(\"nüí° Observa√ß√µes importantes:\")\n",
    "    print(\"‚Ä¢ Transfer√™ncia CPU‚ÜîGPU √© custosa\")\n",
    "    print(\"‚Ä¢ Speedup real depende do tamanho do problema\")\n",
    "    print(\"‚Ä¢ GPU √© mais eficiente para problemas grandes\")\n",
    "    print(\"‚Ä¢ CuPy = np ‚Üí cp (mudan√ßa m√≠nima de c√≥digo)\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  CuPy n√£o dispon√≠vel. Exemplo conceitual:\")\n",
    "    print(\"\"\"\n",
    "    # NumPy (CPU)\n",
    "    import numpy as np\n",
    "    a = np.arange(10_000_000)\n",
    "    b = np.arange(10_000_000)\n",
    "    c = a + b  # Executa no CPU\n",
    "    \n",
    "    # CuPy (GPU) - mudan√ßa m√≠nima!\n",
    "    import cupy as cp\n",
    "    a = cp.arange(10_000_000)  # Criado diretamente na GPU\n",
    "    b = cp.arange(10_000_000)\n",
    "    c = a + b  # Executa na GPU automaticamente!\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a488a8",
   "metadata": {},
   "source": [
    "## 2. Exemplo 11: Multiplica√ß√£o de Matrizes Massivas\n",
    "\n",
    "Para matrizes grandes, GPUs mostram seu verdadeiro poder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab95cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUPY_AVAILABLE:\n",
    "    print(\"üßÆ Exemplo 11: Multiplica√ß√£o de Matrizes Massivas\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Testar diferentes tamanhos de matrizes\n",
    "    matrix_sizes = [1024, 2048, 4096]\n",
    "    \n",
    "    for size in matrix_sizes:\n",
    "        print(f\"nüìä Matrizes {size}x{size} (elementos: {size**2:,})\")\n",
    "        \n",
    "        # Criar matrizes aleat√≥rias\n",
    "        print(\"  Gerando matrizes aleat√≥rias...\")\n",
    "        np.random.seed(42)\n",
    "        A_cpu = np.random.randn(size, size).astype(np.float32)\n",
    "        B_cpu = np.random.randn(size, size).astype(np.float32)\n",
    "        \n",
    "        # NumPy (CPU) com BLAS otimizado\n",
    "        print(\"  Executando multiplica√ß√£o no CPU...\")\n",
    "        start = time.perf_counter()\n",
    "        C_cpu = np.dot(A_cpu, B_cpu)\n",
    "        time_cpu = time.perf_counter() - start\n",
    "        print(f\"  NumPy (CPU):     {time_cpu:.4f}s\")\n",
    "        \n",
    "        # Transferir para GPU\n",
    "        start = time.perf_counter()\n",
    "        A_gpu = cp.asarray(A_cpu)\n",
    "        B_gpu = cp.asarray(B_cpu)\n",
    "        time_transfer_to = time.perf_counter() - start\n",
    "        print(f\"  Transfer√™ncia‚ÜíGPU: {time_transfer_to:.4f}s\")\n",
    "        \n",
    "        # CuPy (GPU)\n",
    "        print(\"  Executando multiplica√ß√£o na GPU...\")\n",
    "        start = time.perf_counter()\n",
    "        C_gpu = cp.dot(A_gpu, B_gpu)\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        time_gpu = time.perf_counter() - start\n",
    "        print(f\"  CuPy (GPU):      {time_gpu:.4f}s\")\n",
    "        \n",
    "        # Transferir resultado de volta\n",
    "        start = time.perf_counter()\n",
    "        C_gpu_cpu = cp.asnumpy(C_gpu)\n",
    "        time_transfer_back = time.perf_counter() - start\n",
    "        print(f\"  Transfer√™ncia‚ÜêGPU: {time_transfer_back:.4f}s\")\n",
    "        \n",
    "        # Verificar precis√£o\n",
    "        max_error = np.max(np.abs(C_cpu - C_gpu_cpu))\n",
    "        are_close = np.allclose(C_cpu, C_gpu_cpu, rtol=1e-4)\n",
    "        \n",
    "        # Calcular speedups\n",
    "        speedup_compute = time_cpu / time_gpu\n",
    "        total_gpu_time = time_transfer_to + time_gpu + time_transfer_back\n",
    "        speedup_total = time_cpu / total_gpu_time\n",
    "        \n",
    "        # Calcular FLOPS (opera√ß√µes de ponto flutuante por segundo)\n",
    "        flops = 2 * size**3  # Multiplica√ß√£o de matrizes: 2*n¬≥ opera√ß√µes\n",
    "        gflops_cpu = flops / (time_cpu * 1e9)\n",
    "        gflops_gpu = flops / (time_gpu * 1e9)\n",
    "        \n",
    "        print(f\"n  üìà Performance:\")\n",
    "        print(f\"    CPU GFLOPS:      {gflops_cpu:.2f}\")\n",
    "        print(f\"    GPU GFLOPS:      {gflops_gpu:.2f}\")\n",
    "        print(f\"    Speedup (comp):  {speedup_compute:.2f}x\")\n",
    "        print(f\"    Speedup (total): {speedup_total:.2f}x\")\n",
    "        print(f\"    Erro m√°ximo:     {max_error:.2e}\")\n",
    "        print(f\"    Precis√£o:        {'‚úì' if are_close else '‚úó'}\")\n",
    "        \n",
    "        # Limpeza de mem√≥ria GPU\n",
    "        del A_gpu, B_gpu, C_gpu\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "    \n",
    "    print(\"nüí° Observa√ß√µes:\")\n",
    "    print(\"‚Ä¢ GPU acelera significativamente opera√ß√µes matriciais grandes\")\n",
    "    print(\"‚Ä¢ GFLOPS (Giga FLOPS) mede efici√™ncia computacional\")\n",
    "    print(\"‚Ä¢ Speedup melhora com tamanho do problema\")\n",
    "    print(\"‚Ä¢ Essencial considerar overhead de transfer√™ncia\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Demonstra√ß√£o conceitual de multiplica√ß√£o de matrizes GPU:\")\n",
    "    \n",
    "    # Simular performance t√≠pica\n",
    "    sizes = [1024, 2048, 4096]\n",
    "    print(\"Speedups t√≠picos para multiplica√ß√£o de matrizes:\")\n",
    "    print(\"Tamanho    CPU (s)    GPU (s)    Speedup\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for size in sizes:\n",
    "        # Estimativas baseadas em hardware t√≠pico\n",
    "        flops = 2 * size**3\n",
    "        time_cpu_est = flops / (100e9)  # ~100 GFLOPS CPU\n",
    "        time_gpu_est = flops / (2000e9)  # ~2000 GFLOPS GPU\n",
    "        speedup_est = time_cpu_est / time_gpu_est\n",
    "        \n",
    "        print(f\"{size:>6}x{size:<6} {time_cpu_est:.3f}     {time_gpu_est:.4f}    {speedup_est:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8054d6",
   "metadata": {},
   "source": [
    "## 3. Introdu√ß√£o ao Numba CUDA\n",
    "\n",
    "Numba CUDA permite escrever kernels customizados para m√°ximo controle sobre a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4031c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if NUMBA_CUDA_AVAILABLE:\n",
    "    print(\"üîß Introdu√ß√£o ao Numba CUDA\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    @cuda.jit\n",
    "    def add_kernel(a, b, c):\n",
    "        # \"\"\"Kernel CUDA para soma elemento por elemento\"\"\"\n",
    "        # Obter √≠ndice do thread atual\n",
    "        i = cuda.grid(1)\n",
    "        \n",
    "        # Verificar bounds\n",
    "        if i < a.size:\n",
    "            c[i] = a[i] + b[i]\n",
    "    \n",
    "    @cuda.jit \n",
    "    def square_kernel(arr, result):\n",
    "        # \"\"\"Kernel CUDA para elevar ao quadrado\"\"\"\n",
    "        i = cuda.grid(1)\n",
    "        \n",
    "        if i < arr.size:\n",
    "            result[i] = arr[i] * arr[i]\n",
    "    \n",
    "    # Exemplo 12: Monte Carlo œÄ com CUDA\n",
    "    @cuda.jit\n",
    "    def monte_carlo_pi_kernel(rng_states, n_per_thread, results):\n",
    "        # \"\"\"Kernel para estimativa de œÄ por Monte Carlo\"\"\"\n",
    "        thread_id = cuda.grid(1)\n",
    "        \n",
    "        if thread_id >= rng_states.size:\n",
    "            return\n",
    "            \n",
    "        # Cada thread conta pontos dentro do c√≠rculo\n",
    "        count = 0\n",
    "        for i in range(n_per_thread):\n",
    "            x = cuda.random.xoroshiro128p_uniform_float32(rng_states, thread_id)\n",
    "            y = cuda.random.xoroshiro128p_uniform_float32(rng_states, thread_id)\n",
    "            \n",
    "            if x*x + y*y <= 1.0:\n",
    "                count += 1\n",
    "        \n",
    "        results[thread_id] = count\n",
    "    \n",
    "    def monte_carlo_pi_cuda(n_samples, n_threads=256):\n",
    "        # \"\"\"Estimativa de œÄ usando CUDA\"\"\"\n",
    "        n_blocks = (n_samples + n_threads - 1) // n_threads\n",
    "        samples_per_thread = n_samples // (n_blocks * n_threads)\n",
    "        \n",
    "        # Alocar arrays na GPU\n",
    "        rng_states = cuda.random.create_xoroshiro128p_states(n_blocks * n_threads, seed=42)\n",
    "        results = cuda.device_array(n_blocks * n_threads, dtype=np.int32)\n",
    "        \n",
    "        # Executar kernel\n",
    "        start = time.perf_counter()\n",
    "        monte_carlo_pi_kernel[n_blocks, n_threads](rng_states, samples_per_thread, results)\n",
    "        cuda.synchronize()\n",
    "        time_gpu = time.perf_counter() - start\n",
    "        \n",
    "        # Transferir resultados e somar\n",
    "        results_host = results.copy_to_host()\n",
    "        total_inside = np.sum(results_host)\n",
    "        total_samples = samples_per_thread * n_blocks * n_threads\n",
    "        \n",
    "        pi_estimate = 4.0 * total_inside / total_samples\n",
    "        return pi_estimate, time_gpu\n",
    "    \n",
    "    print(\"nüé≤ Exemplo 12: Monte Carlo œÄ com Numba CUDA\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    # Teste com diferentes n√∫meros de amostras\n",
    "    sample_counts = [1_000_000, 10_000_000, 100_000_000]\n",
    "    \n",
    "    for n_samples in sample_counts:\n",
    "        print(f\"nAmostras: {n_samples:,}\")\n",
    "        \n",
    "        # Vers√£o CPU para compara√ß√£o\n",
    "        start = time.perf_counter()\n",
    "        np.random.seed(42)\n",
    "        x_cpu = np.random.uniform(-1, 1, n_samples)\n",
    "        y_cpu = np.random.uniform(-1, 1, n_samples)\n",
    "        inside_cpu = np.sum((x_cpu**2 + y_cpu**2) <= 1)\n",
    "        pi_cpu = 4 * inside_cpu / n_samples\n",
    "        time_cpu = time.perf_counter() - start\n",
    "        \n",
    "        # Vers√£o GPU\n",
    "        pi_gpu, time_gpu = monte_carlo_pi_cuda(n_samples)\n",
    "        \n",
    "        speedup = time_cpu / time_gpu\n",
    "        error_cpu = abs(pi_cpu - np.pi)\n",
    "        error_gpu = abs(pi_gpu - np.pi)\n",
    "        \n",
    "        print(f\"  CPU: œÄ ‚âà {pi_cpu:.6f}, erro = {error_cpu:.6f}, tempo = {time_cpu:.4f}s\")\n",
    "        print(f\"  GPU: œÄ ‚âà {pi_gpu:.6f}, erro = {error_gpu:.6f}, tempo = {time_gpu:.4f}s\")\n",
    "        print(f\"  Speedup: {speedup:.2f}x\")\n",
    "    \n",
    "    print(\"nüí° Conceitos CUDA importantes:\")\n",
    "    print(\"‚Ä¢ Grid: cole√ß√£o de blocos\")\n",
    "    print(\"‚Ä¢ Block: cole√ß√£o de threads\")\n",
    "    print(\"‚Ä¢ Thread: unidade de execu√ß√£o\")\n",
    "    print(\"‚Ä¢ cuda.grid(1): obt√©m √≠ndice global do thread\")\n",
    "    print(\"‚Ä¢ cuda.synchronize(): aguarda conclus√£o\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Numba CUDA n√£o dispon√≠vel. Conceitos fundamentais:\")\n",
    "    print(\"\"\"\n",
    "    Hierarquia CUDA:\n",
    "    \n",
    "    Grid (toda a GPU)\n",
    "    ‚îú‚îÄ‚îÄ Block 0 (grupo de threads)\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ Thread 0\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ Thread 1\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ Thread N\n",
    "    ‚îú‚îÄ‚îÄ Block 1\n",
    "    ‚îî‚îÄ‚îÄ Block M\n",
    "    \n",
    "    Kernel CUDA exemplo:\n",
    "    \n",
    "    @cuda.jit\n",
    "    def meu_kernel(input_array, output_array):\n",
    "        # Obter √≠ndice √∫nico para este thread\n",
    "        i = cuda.grid(1)\n",
    "        \n",
    "        # Verificar bounds\n",
    "        if i < input_array.size:\n",
    "            output_array[i] = input_array[i] * 2\n",
    "    \n",
    "    # Lan√ßar kernel\n",
    "    threads_per_block = 256\n",
    "    blocks_per_grid = (array.size + threads_per_block - 1) // threads_per_block\n",
    "    meu_kernel[blocks_per_grid, threads_per_block](input_arr, output_arr)\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8962921b",
   "metadata": {},
   "source": [
    "## 4. Exemplo 13: Simula√ß√£o de Difus√£o de Calor 2D\n",
    "\n",
    "Vamos implementar uma simula√ß√£o real√≠stica de engenharia: difus√£o de calor em uma placa met√°lica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e73a39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heat_equation_cpu(T, alpha, dx, dy, dt, steps):\n",
    "    \"\"\"\n",
    "    Simula difus√£o de calor 2D no CPU\n",
    "    Equa√ß√£o: ‚àÇT/‚àÇt = Œ±(‚àÇ¬≤T/‚àÇx¬≤ + ‚àÇ¬≤T/‚àÇy¬≤)\n",
    "    \"\"\"\n",
    "    ny, nx = T.shape\n",
    "    T_new = T.copy()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    for step in range(steps):\n",
    "        # Atualizar pontos internos usando diferen√ßas finitas\n",
    "        for i in range(1, ny-1):\n",
    "            for j in range(1, nx-1):\n",
    "                T_new[i, j] = T[i, j] + alpha * dt * (\n",
    "                    (T[i+1, j] - 2*T[i, j] + T[i-1, j]) / dx**2 +\n",
    "                    (T[i, j+1] - 2*T[i, j] + T[i, j-1]) / dy**2\n",
    "                )\n",
    "        \n",
    "        # Trocar arrays\n",
    "        T, T_new = T_new, T\n",
    "    \n",
    "    exec_time = time.perf_counter() - start\n",
    "    return T, exec_time\n",
    "\n",
    "if CUPY_AVAILABLE:\n",
    "    def heat_equation_gpu(T_gpu, alpha, dx, dy, dt, steps):\n",
    "        \"\"\"Simula difus√£o de calor 2D na GPU usando CuPy\"\"\"\n",
    "        ny, nx = T_gpu.shape\n",
    "        T_new_gpu = cp.copy(T_gpu)\n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        \n",
    "        for step in range(steps):\n",
    "            # Atualizar usando slicing vetorizado\n",
    "            T_new_gpu[1:-1, 1:-1] = T_gpu[1:-1, 1:-1] + alpha * dt * (\n",
    "                (T_gpu[2:, 1:-1] - 2*T_gpu[1:-1, 1:-1] + T_gpu[:-2, 1:-1]) / dx**2 +\n",
    "                (T_gpu[1:-1, 2:] - 2*T_gpu[1:-1, 1:-1] + T_gpu[1:-1, :-2]) / dy**2\n",
    "            )\n",
    "            \n",
    "            # Trocar arrays\n",
    "            T_gpu, T_new_gpu = T_new_gpu, T_gpu\n",
    "            \n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        exec_time = time.perf_counter() - start\n",
    "        return T_gpu, exec_time\n",
    "\n",
    "if NUMBA_CUDA_AVAILABLE:\n",
    "    @cuda.jit\n",
    "    def heat_equation_kernel(T_old, T_new, alpha, dx, dy, dt):\n",
    "        \"\"\"Kernel CUDA para um passo da equa√ß√£o de calor\"\"\"\n",
    "        i, j = cuda.grid(2)  # Obter coordenadas 2D\n",
    "        \n",
    "        ny, nx = T_old.shape\n",
    "        \n",
    "        # Verificar bounds (evitar bordas)\n",
    "        if 1 <= i < ny-1 and 1 <= j < nx-1:\n",
    "            T_new[i, j] = T_old[i, j] + alpha * dt * (\n",
    "                (T_old[i+1, j] - 2*T_old[i, j] + T_old[i-1, j]) / (dx*dx) +\n",
    "                (T_old[i, j+1] - 2*T_old[i, j] + T_old[i, j-1]) / (dy*dy)\n",
    "            )\n",
    "    \n",
    "    def heat_equation_cuda(T_host, alpha, dx, dy, dt, steps):\n",
    "        \"\"\"Simula difus√£o de calor 2D usando kernels CUDA\"\"\"\n",
    "        ny, nx = T_host.shape\n",
    "        \n",
    "        # Alocar na GPU\n",
    "        T_gpu = cuda.to_device(T_host)\n",
    "        T_new_gpu = cuda.device_array_like(T_gpu)\n",
    "        \n",
    "        # Configurar grid de threads\n",
    "        threads_per_block = (16, 16)\n",
    "        blocks_per_grid_x = (nx + threads_per_block[0] - 1) // threads_per_block[0]\n",
    "        blocks_per_grid_y = (ny + threads_per_block[1] - 1) // threads_per_block[1]\n",
    "        blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)\n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        \n",
    "        for step in range(steps):\n",
    "            heat_equation_kernel[blocks_per_grid, threads_per_block](\n",
    "                T_gpu, T_new_gpu, alpha, dx, dy, dt\n",
    "            )\n",
    "            \n",
    "            # Trocar buffers\n",
    "            T_gpu, T_new_gpu = T_new_gpu, T_gpu\n",
    "        \n",
    "        cuda.synchronize()\n",
    "        exec_time = time.perf_counter() - start\n",
    "        \n",
    "        # Transferir resultado de volta\n",
    "        result = T_gpu.copy_to_host()\n",
    "        return result, exec_time\n",
    "\n",
    "print(\"üå°Ô∏è  Exemplo 13: Simula√ß√£o de Difus√£o de Calor 2D\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Par√¢metros f√≠sicos\n",
    "Lx, Ly = 1.0, 1.0  # Dimens√µes da placa (metros)\n",
    "nx, ny = 256, 256  # Pontos da malha\n",
    "dx = Lx / (nx - 1)\n",
    "dy = Ly / (ny - 1)\n",
    "alpha = 1e-4  # Difusividade t√©rmica (m¬≤/s)\n",
    "dt = 0.25 * min(dx, dy)**2 / (4 * alpha)  # Passo de tempo est√°vel\n",
    "steps = 500  # N√∫mero de passos de tempo\n",
    "\n",
    "print(f\"Par√¢metros da simula√ß√£o:\")\n",
    "print(f\"  Malha: {nx}x{ny} = {nx*ny:,} pontos\")\n",
    "print(f\"  Passos de tempo: {steps}\")\n",
    "print(f\"  dt = {dt:.2e}s (estabilidade: {dt*alpha/(dx**2):.3f} < 0.25)\")\n",
    "\n",
    "# Condi√ß√µes iniciais e de contorno\n",
    "T_initial = np.zeros((ny, nx), dtype=np.float32)\n",
    "\n",
    "# Fonte de calor no centro\n",
    "center_x, center_y = nx//2, ny//2\n",
    "radius = min(nx, ny) // 8\n",
    "for i in range(ny):\n",
    "    for j in range(nx):\n",
    "        if (i - center_y)**2 + (j - center_x)**2 <= radius**2:\n",
    "            T_initial[i, j] = 100.0  # 100¬∞C\n",
    "\n",
    "# Bordas mantidas a 0¬∞C (condi√ß√£o de Dirichlet)\n",
    "T_initial[0, :] = T_initial[-1, :] = 0.0\n",
    "T_initial[:, 0] = T_initial[:, -1] = 0.0\n",
    "\n",
    "print(f\"nCondi√ß√µes iniciais: fonte de calor central a 100¬∞C\")\n",
    "\n",
    "# Simular no CPU\n",
    "print(\"n‚è±Ô∏è  Executando simula√ß√£o no CPU...\")\n",
    "T_cpu, time_cpu = heat_equation_cpu(T_initial.copy(), alpha, dx, dy, dt, steps)\n",
    "print(f\"CPU: {time_cpu:.3f}s\")\n",
    "\n",
    "# Simular na GPU (se dispon√≠vel)\n",
    "if CUPY_AVAILABLE:\n",
    "    print(\"n‚è±Ô∏è  Executando simula√ß√£o na GPU (CuPy)...\")\n",
    "    T_gpu_cupy, time_gpu_cupy = heat_equation_gpu(cp.asarray(T_initial), alpha, dx, dy, dt, steps)\n",
    "    T_gpu_cupy_host = cp.asnumpy(T_gpu_cupy)\n",
    "    \n",
    "    speedup_cupy = time_cpu / time_gpu_cupy\n",
    "    max_diff_cupy = np.max(np.abs(T_cpu - T_gpu_cupy_host))\n",
    "    \n",
    "    print(f\"GPU (CuPy): {time_gpu_cupy:.3f}s, speedup: {speedup_cupy:.2f}x\")\n",
    "    print(f\"Diferen√ßa m√°xima CPU vs GPU: {max_diff_cupy:.2e}¬∞C\")\n",
    "\n",
    "if NUMBA_CUDA_AVAILABLE:\n",
    "    print(\"n‚è±Ô∏è  Executando simula√ß√£o na GPU (Numba CUDA)...\")\n",
    "    T_gpu_cuda, time_gpu_cuda = heat_equation_cuda(T_initial.copy(), alpha, dx, dy, dt, steps)\n",
    "    \n",
    "    speedup_cuda = time_cpu / time_gpu_cuda\n",
    "    max_diff_cuda = np.max(np.abs(T_cpu - T_gpu_cuda))\n",
    "    \n",
    "    print(f\"GPU (CUDA): {time_gpu_cuda:.3f}s, speedup: {speedup_cuda:.2f}x\")\n",
    "    print(f\"Diferen√ßa m√°xima CPU vs CUDA: {max_diff_cuda:.2e}¬∞C\")\n",
    "\n",
    "# Visualiza√ß√£o dos resultados\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Estado inicial\n",
    "im1 = axes[0].imshow(T_initial, cmap='hot', interpolation='bilinear')\n",
    "axes[0].set_title('Estado Inicialn(t = 0)')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "plt.colorbar(im1, ax=axes[0], label='Temperatura (¬∞C)')\n",
    "\n",
    "# Estado final (CPU)\n",
    "im2 = axes[1].imshow(T_cpu, cmap='hot', interpolation='bilinear')\n",
    "axes[1].set_title(f'Estado Final (CPU)n(t = {steps*dt:.3f}s)')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('y')\n",
    "plt.colorbar(im2, ax=axes[1], label='Temperatura (¬∞C)')\n",
    "\n",
    "# Compara√ß√£o GPU vs CPU (se dispon√≠vel)\n",
    "if CUPY_AVAILABLE:\n",
    "    diff = T_gpu_cupy_host - T_cpu\n",
    "    im3 = axes[2].imshow(diff, cmap='RdBu_r', interpolation='bilinear')\n",
    "    axes[2].set_title('Diferen√ßa GPU - CPUn(CuPy)')\n",
    "    axes[2].set_xlabel('x')\n",
    "    axes[2].set_ylabel('y')\n",
    "    plt.colorbar(im3, ax=axes[2], label='Diferen√ßa (¬∞C)')\n",
    "else:\n",
    "    # Perfil de temperatura no centro\n",
    "    center_line = T_cpu[ny//2, :]\n",
    "    axes[2].plot(np.linspace(0, Lx, nx), center_line, 'r-', linewidth=2)\n",
    "    axes[2].set_title('Perfil de Temperaturan(linha central)')\n",
    "    axes[2].set_xlabel('Posi√ß√£o x (m)')\n",
    "    axes[2].set_ylabel('Temperatura (¬∞C)')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"nüí° Aplica√ß√µes em Engenharia Civil:\")\n",
    "print(\"‚Ä¢ An√°lise t√©rmica de estruturas de concreto\")\n",
    "print(\"‚Ä¢ Simula√ß√£o de inc√™ndios em edifica√ß√µes\")\n",
    "print(\"‚Ä¢ Comportamento t√©rmico de pavimentos\")\n",
    "print(\"‚Ä¢ Isolamento t√©rmico de edif√≠cios\")\n",
    "print(\"‚Ä¢ Pontes t√©rmicas em estruturas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0022b75",
   "metadata": {},
   "source": [
    "## 5. Integra√ß√£o CPU + GPU e Ferramentas Avan√ßadas\n",
    "\n",
    "Vamos ver como combinar CPU e GPU eficientemente e explorar ferramentas do ecossistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a516aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîó Integra√ß√£o CPU + GPU e Ecossistema HPC\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "print(\"nüìö Ferramentas Avan√ßadas do Ecossistema:\")\n",
    "print(\"n1. üöÄ **Dask + CuPy**: Computa√ß√£o distribu√≠da em m√∫ltiplas GPUs\")\n",
    "print(\"   ‚Ä¢ Escalabilidade para clusters\")\n",
    "print(\"   ‚Ä¢ DataFrame distribu√≠dos\")\n",
    "print(\"   ‚Ä¢ Exemplo: `import dask.array as da; da.from_array(cupy_array)`\")\n",
    "\n",
    "print(\"n2. üß† **PyTorch/TensorFlow**: Machine Learning em larga escala\")\n",
    "print(\"   ‚Ä¢ Redes neurais para previs√£o estrutural\")\n",
    "print(\"   ‚Ä¢ Processamento de imagens de inspe√ß√£o\")\n",
    "print(\"   ‚Ä¢ Detec√ß√£o de anomalias em estruturas\")\n",
    "\n",
    "print(\"n3. üê≥ **Singularity/Docker**: Containers para HPC\")\n",
    "print(\"   ‚Ä¢ Reprodutibilidade em clusters\")\n",
    "print(\"   ‚Ä¢ Isolamento de depend√™ncias\")\n",
    "print(\"   ‚Ä¢ Portabilidade entre sistemas\")\n",
    "\n",
    "print(\"n4. üìä **Rapids**: Acelera√ß√£o completa do pipeline de dados\")\n",
    "print(\"   ‚Ä¢ cuDF (pandas para GPU)\")\n",
    "print(\"   ‚Ä¢ cuML (scikit-learn para GPU)\")\n",
    "print(\"   ‚Ä¢ cuGraph (NetworkX para GPU)\")\n",
    "\n",
    "# Demonstra√ß√£o de pipeline h√≠brido CPU+GPU\n",
    "def hybrid_data_processing_example():\n",
    "    \n",
    "    print(\"nüí° Exemplo de Pipeline H√≠brido CPU+GPU:\")\n",
    "    print(\"n1. **Pr√©-processamento no CPU**:\")\n",
    "    print(\"   ‚Ä¢ Leitura de arquivos\")\n",
    "    print(\"   ‚Ä¢ Parsing de dados\")\n",
    "    print(\"   ‚Ä¢ Valida√ß√£o e limpeza\")\n",
    "    \n",
    "    print(\"n2. **Computa√ß√£o intensiva na GPU**:\")\n",
    "    print(\"   ‚Ä¢ Simula√ß√µes num√©ricas\")\n",
    "    print(\"   ‚Ä¢ Opera√ß√µes matriciais massivas\")\n",
    "    print(\"   ‚Ä¢ Processamento paralelo\")\n",
    "    \n",
    "    print(\"n3. **P√≥s-processamento no CPU**:\")\n",
    "    print(\"   ‚Ä¢ An√°lise estat√≠stica\")\n",
    "    print(\"   ‚Ä¢ Gera√ß√£o de relat√≥rios\")\n",
    "    print(\"   ‚Ä¢ Visualiza√ß√£o\")\n",
    "    \n",
    "    if CUPY_AVAILABLE:\n",
    "        print(\"nüîß Implementa√ß√£o pr√°tica:\")\n",
    "        \n",
    "        # Simular dados de entrada (CPU)\n",
    "        print(\"   Gerando dados no CPU...\")\n",
    "        data_cpu = np.random.randn(1000, 1000).astype(np.float32)\n",
    "        \n",
    "        # Transferir para GPU para processamento intensivo\n",
    "        print(\"   Transferindo para GPU...\")\n",
    "        data_gpu = cp.asarray(data_cpu)\n",
    "        \n",
    "        # Opera√ß√µes intensivas na GPU\n",
    "        print(\"   Processando na GPU...\")\n",
    "        start = time.perf_counter()\n",
    "        \n",
    "        # Simula√ß√£o: m√∫ltiplas opera√ß√µes matriciais\n",
    "        result_gpu = cp.dot(data_gpu, data_gpu.T)\n",
    "        result_gpu = cp.linalg.eigvals(result_gpu)\n",
    "        result_gpu = cp.sort(result_gpu)\n",
    "        \n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        gpu_time = time.perf_counter() - start\n",
    "        \n",
    "        # Transferir de volta para an√°lise no CPU\n",
    "        print(\"   Transferindo resultado de volta...\")\n",
    "        result_cpu = cp.asnumpy(result_gpu)\n",
    "        \n",
    "        # An√°lise final no CPU\n",
    "        print(\"   Analisando no CPU...\")\n",
    "        mean_eigenval = np.mean(result_cpu.real)\n",
    "        std_eigenval = np.std(result_cpu.real)\n",
    "        \n",
    "        print(f\"nüìä Resultados:\")\n",
    "        print(f\"   Tempo GPU: {gpu_time:.4f}s\")\n",
    "        print(f\"   Autovalor m√©dio: {mean_eigenval:.6f}\")\n",
    "        print(f\"   Desvio padr√£o: {std_eigenval:.6f}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"n‚ö†Ô∏è  CuPy n√£o dispon√≠vel para demonstra√ß√£o pr√°tica\")\n",
    "\n",
    "hybrid_data_processing_example()\n",
    "\n",
    "print(\"nüéØ Boas Pr√°ticas para CPU+GPU:\")\n",
    "print(\"n‚Ä¢ **Minimize transfer√™ncias**: Mantenha dados na GPU o m√°ximo poss√≠vel\")\n",
    "print(\"‚Ä¢ **Sobreposi√ß√£o**: Use streams CUDA para computa√ß√£o e transfer√™ncia simult√¢neas\")\n",
    "print(\"‚Ä¢ **Pinned memory**: Acelera transfer√™ncias CPU‚ÜîGPU\")\n",
    "print(\"‚Ä¢ **Batch processing**: Processe m√∫ltiplos itens juntos\")\n",
    "print(\"‚Ä¢ **Profiling**: Use nvprof, Nsight para identificar gargalos\")\n",
    "\n",
    "print(\"nüìà Quando usar GPU vs CPU:\")\n",
    "print(\"n**Use GPU quando:**\")\n",
    "print(\"‚Ä¢ Alto paralelismo (milhares de opera√ß√µes simult√¢neas)\")\n",
    "print(\"‚Ä¢ Opera√ß√µes matem√°ticas intensivas\")\n",
    "print(\"‚Ä¢ Dados regulares e homog√™neos\")\n",
    "print(\"‚Ä¢ Pouca ramifica√ß√£o condicional\")\n",
    "\n",
    "print(\"n**Use CPU quando:**\")\n",
    "print(\"‚Ä¢ C√≥digo sequencial complexo\")\n",
    "print(\"‚Ä¢ Muitas ramifica√ß√µes condicionais\")\n",
    "print(\"‚Ä¢ I/O intensivo\")\n",
    "print(\"‚Ä¢ Processamento de strings/texto\")\n",
    "\n",
    "print(\"nüîÆ Futuro da Computa√ß√£o Heterog√™nea:\")\n",
    "print(\"‚Ä¢ **SYCL/DPC++**: Programa√ß√£o unificada CPU/GPU/FPGA\")\n",
    "print(\"‚Ä¢ **Intel oneAPI**: Toolchain multi-arquitetura\")\n",
    "print(\"‚Ä¢ **AMD HIP**: Portabilidade CUDA‚ÜíROCm\")\n",
    "print(\"‚Ä¢ **WebGPU**: Computa√ß√£o GPU no navegador\")\n",
    "\n",
    "# Exemplo de medi√ß√£o de throughput\n",
    "if CUPY_AVAILABLE:\n",
    "    print(\"nüìä Benchmark Final: Throughput de Dados\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    data_sizes = [1, 10, 100]  # MB\n",
    "    \n",
    "    for size_mb in data_sizes:\n",
    "        n_elements = (size_mb * 1024 * 1024) // 4  # 4 bytes por float32\n",
    "        \n",
    "        # CPU\n",
    "        data_cpu = np.random.randn(n_elements).astype(np.float32)\n",
    "        start = time.perf_counter()\n",
    "        result_cpu = np.sum(data_cpu**2)\n",
    "        time_cpu = time.perf_counter() - start\n",
    "        throughput_cpu = size_mb / time_cpu\n",
    "        \n",
    "        # GPU\n",
    "        data_gpu = cp.asarray(data_cpu)\n",
    "        start = time.perf_counter()\n",
    "        result_gpu = cp.sum(data_gpu**2)\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        time_gpu = time.perf_counter() - start\n",
    "        throughput_gpu = size_mb / time_gpu\n",
    "        \n",
    "        print(f\"{size_mb:3d} MB: CPU {throughput_cpu:6.1f} MB/s, GPU {throughput_gpu:6.1f} MB/s\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
