# âš¡ Aula 3 â€“ GPUs em Python e AplicaÃ§Ãµes em Engenharia

**ComputaÃ§Ã£o de Alto Desempenho em Python para Engenharia Civil**

---

## ğŸ“‹ Agenda da Aula

1. **Arquitetura GPU** vs CPU
2. **CUDA Programming Model**
3. **CuPy** - NumPy para GPU
4. **Numba CUDA** - Kernels Customizados
5. **Memory Management** e OtimizaÃ§Ã£o
6. **AplicaÃ§Ãµes PrÃ¡ticas** em Engenharia
7. **Performance Analysis** CPU vs GPU
8. **LimitaÃ§Ãµes** e ConsideraÃ§Ãµes

---

## ğŸ–¥ï¸ CPU vs GPU: Arquiteturas Complementares

### CPU - Central Processing Unit

#### CaracterÃ­sticas
- **4-64 nÃºcleos** complexos
- **Alta frequÃªncia** (3-5 GHz)
- **Cache hierarchy** sofisticada (L1, L2, L3)
- **Branch prediction** avanÃ§ada
- **Out-of-order execution**

#### Filosofia: **Latency Optimized**
```
Objetivo: Executar tarefas individuais o mais rÃ¡pido possÃ­vel
EstratÃ©gia: NÃºcleos poderosos + cache inteligente
```

#### AplicaÃ§Ãµes Ideais
- CÃ³digo **sequencial** complexo
- **Branch-heavy** algorithms
- **Single-threaded** performance crÃ­tica
- **Low-latency** requirements

### GPU - Graphics Processing Unit

#### CaracterÃ­sticas
- **1000+ nÃºcleos** simples
- **FrequÃªncia menor** (~1-2 GHz)
- **MemÃ³ria alta velocidade** (HBM)
- **SIMD architecture** (Single Instruction, Multiple Data)
- **Massive parallelism**

#### Filosofia: **Throughput Optimized**
```
Objetivo: Executar milhares de tarefas simples simultaneamente
EstratÃ©gia: Muitos nÃºcleos simples + alta largura de banda
```

#### AplicaÃ§Ãµes Ideais
- **Data parallel** algorithms
- **Regular memory access** patterns
- **Compute intensive** workloads
- **High throughput** scenarios

![CPU vs GPU Architecture](figures/cpu_gpu_architecture.png)

---

## ğŸ”§ CUDA Programming Model

### Hierarquia de ExecuÃ§Ã£o

#### Grid â†’ Blocks â†’ Threads
```
Grid (GPU inteira)
â”œâ”€â”€ Block 0 (Streaming Multiprocessor)
â”‚   â”œâ”€â”€ Thread 0, 1, 2, ..., 1023
â”œâ”€â”€ Block 1
â”‚   â”œâ”€â”€ Thread 0, 1, 2, ..., 1023
â””â”€â”€ Block N
    â”œâ”€â”€ Thread 0, 1, 2, ..., 1023
```

![CUDA Hierarchy](figures/cuda_hierarchy.png)

#### CorrespondÃªncia com Hardware
- **Grid**: Problema inteiro na GPU
- **Block**: Grupo de threads em um SM
- **Thread**: Unidade bÃ¡sica de execuÃ§Ã£o

### Hierarquia de MemÃ³ria

```
Threads â†’ Shared Memory (fast, per-block)
       â†’ Global Memory (slow, all threads)
       â†’ Constant Memory (cached, read-only)
       â†’ Texture Memory (cached, spatial locality)
```

#### Velocidades Relativas
| Tipo | LatÃªncia | Largura de Banda |
|------|----------|------------------|
| **Registers** | 1 ciclo | ~8 TB/s |
| **Shared Memory** | ~20 ciclos | ~1.5 TB/s |
| **Global Memory** | ~200-400 ciclos | ~900 GB/s |
| **CPU RAM** | ~100-300 ciclos | ~100 GB/s |

![GPU Memory Hierarchy](figures/gpu_memory_hierarchy.png)

### Modelo de ProgramaÃ§Ã£o

#### SIMT (Single Instruction, Multiple Threads)
```python
# Conceito: mesma instruÃ§Ã£o executada por muitas threads
@cuda.jit
def vector_add(a, b, c):
    # Cada thread processa um elemento
    idx = cuda.grid(1)  # ID Ãºnica da thread
    if idx < len(a):
        c[idx] = a[idx] + b[idx]
        
# LanÃ§amento: 1024 threads por block
threads_per_block = 1024
blocks_per_grid = (len(a) + threads_per_block - 1) // threads_per_block
vector_add[blocks_per_grid, threads_per_block](a, b, c)
```

---

## ğŸ“Š CuPy - NumPy para GPU

### O que Ã© CuPy?

- **NumPy-compatible** array library para GPU
- **Drop-in replacement** para muitas operaÃ§Ãµes NumPy
- **Automatic memory management** 
- **Kernels otimizados** para operaÃ§Ãµes comuns

### Sintaxe Familiar

```python
import numpy as np
import cupy as cp

# CPU (NumPy)
a_cpu = np.random.rand(1000, 1000)
b_cpu = np.random.rand(1000, 1000)
c_cpu = np.dot(a_cpu, b_cpu)

# GPU (CuPy) - MESMA SINTAXE!
a_gpu = cp.random.rand(1000, 1000)
b_gpu = cp.random.rand(1000, 1000)
c_gpu = cp.dot(a_gpu, b_gpu)
```

### TransferÃªncia de Dados

```python
# CPU â†’ GPU
a_cpu = np.array([1, 2, 3, 4])
a_gpu = cp.asarray(a_cpu)

# GPU â†’ CPU  
result_cpu = cp.asnumpy(a_gpu)

# OperaÃ§Ãµes in-place na GPU
a_gpu *= 2  # Permanece na GPU
```

### AplicaÃ§Ãµes Diretas

#### Ãlgebra Linear
```python
# Sistemas lineares grandes
A = cp.random.rand(10000, 10000)
b = cp.random.rand(10000)
x = cp.linalg.solve(A, b)  # Muito mais rÃ¡pido que NumPy
```

#### FFT (Fast Fourier Transform)
```python
# AnÃ¡lise de sinais sÃ­smicos
signal = cp.random.rand(2**20)  # 1M pontos
fft_result = cp.fft.fft(signal)  # GPU accelerated
spectrum = cp.abs(fft_result)**2
```

#### Image Processing
```python
# Processamento de imagens de inspeÃ§Ã£o
image = cp.asarray(cv2_image)
filtered = cp.ndimage.gaussian_filter(image, sigma=2)
edges = cp.ndimage.sobel(filtered)
```

---

## ğŸ› ï¸ Numba CUDA - Kernels Customizados

### Quando Usar Numba CUDA?

#### CuPy Ã© Suficiente Quando:
- OperaÃ§Ãµes **padrÃ£o** do NumPy
- **Pipelines simples** de transformaÃ§Ãµes
- **Prototipagem** rÃ¡pida

#### Numba CUDA Ã© NecessÃ¡rio Quando:
- **Algoritmos especÃ­ficos** sem equivalente NumPy
- **OtimizaÃ§Ãµes customizadas** de memÃ³ria
- **Controle fino** sobre threads/blocks
- **FusÃ£o** de mÃºltiplas operaÃ§Ãµes

### Decorators BÃ¡sicos

```python
from numba import cuda

@cuda.jit
def simple_kernel(array):
    # Kernel bÃ¡sico - sem otimizaÃ§Ãµes especiais
    idx = cuda.grid(1)
    if idx < len(array):
        array[idx] *= 2

@cuda.jit('void(float32[:], float32[:])')  # Assinatura explÃ­cita
def typed_kernel(a, b):
    # Mais rÃ¡pido - tipos conhecidos em compile-time
    idx = cuda.grid(1)
    if idx < len(a):
        b[idx] = a[idx] * a[idx]
```

### Grid e Block Calculation

```python
def calculate_grid_block(data_size, threads_per_block=256):
    """Calcula grid/block para cobertura completa dos dados"""
    blocks_per_grid = (data_size + threads_per_block - 1) // threads_per_block
    return blocks_per_grid, threads_per_block

# Uso
data = cp.random.rand(10000)
blocks, threads = calculate_grid_block(len(data))
my_kernel[blocks, threads](data)
```

### AplicaÃ§Ã£o: Monte Carlo Ï€

```python
@cuda.jit
def monte_carlo_pi_kernel(rng_states, n_samples, hits):
    """Kernel para estimativa de Ï€ usando Monte Carlo"""
    idx = cuda.grid(1)
    if idx >= len(hits):
        return
    
    local_hits = 0
    for i in range(n_samples):
        # Gerar pontos aleatÃ³rios
        x = curand.uniform(rng_states, idx)
        y = curand.uniform(rng_states, idx)
        
        # Testar se estÃ¡ dentro do cÃ­rculo
        if x*x + y*y <= 1.0:
            local_hits += 1
    
    hits[idx] = local_hits

# LanÃ§amento
n_threads = 1024
n_samples_per_thread = 10000
hits = cp.zeros(n_threads, dtype=cp.int32)

monte_carlo_pi_kernel[32, 32](rng_states, n_samples_per_thread, hits)

# Estimativa final
total_hits = cp.sum(hits)
total_samples = n_threads * n_samples_per_thread
pi_estimate = 4.0 * total_hits / total_samples
```

---

## ğŸ§  Memory Management e OtimizaÃ§Ã£o

### GPU Memory Hierarchy

#### Global Memory
- **Maior** (~8-80 GB)
- **Mais lenta** (~900 GB/s)
- **AcessÃ­vel** por todas as threads
- **PadrÃ£o** para arrays CuPy

#### Shared Memory
- **Menor** (~48-164 KB per block)
- **Muito rÃ¡pida** (~1.5 TB/s)
- **Compartilhada** dentro do block
- **Manual** allocation

#### Exemplo: Matrix Multiplication Otimizada

```python
@cuda.jit
def matmul_shared(A, B, C):
    """MultiplicaÃ§Ã£o de matrizes usando shared memory"""
    # Shared memory por block
    sA = cuda.shared.array((16, 16), numba.float32)
    sB = cuda.shared.array((16, 16), numba.float32)
    
    # Thread e block IDs
    tx = cuda.threadIdx.x
    ty = cuda.threadIdx.y
    bx = cuda.blockIdx.x
    by = cuda.blockIdx.y
    
    # Coordenadas globais
    row = by * 16 + ty
    col = bx * 16 + tx
    
    result = 0.0
    for tile in range((A.shape[1] + 15) // 16):
        # Carregar tile para shared memory
        if row < A.shape[0] and tile * 16 + tx < A.shape[1]:
            sA[ty, tx] = A[row, tile * 16 + tx]
        else:
            sA[ty, tx] = 0.0
            
        if col < B.shape[1] and tile * 16 + ty < B.shape[0]:
            sB[ty, tx] = B[tile * 16 + ty, col]
        else:
            sB[ty, tx] = 0.0
        
        # Sincronizar threads no block
        cuda.syncthreads()
        
        # Computar produto parcial
        for k in range(16):
            result += sA[ty, k] * sB[k, tx]
            
        # Sincronizar antes do prÃ³ximo tile
        cuda.syncthreads()
    
    # Escrever resultado
    if row < C.shape[0] and col < C.shape[1]:
        C[row, col] = result
```

### Memory Coalescing

#### Problema: Acesso NÃ£o-Coalescido
```python
# âŒ Ruim: threads acessam memÃ³ria de forma dispersa
@cuda.jit
def bad_memory_access(data):
    idx = cuda.grid(1)
    if idx < len(data):
        # Threads consecutivas acessam posiÃ§Ãµes distantes
        data[idx * 100] = idx  # Stride = 100
```

#### SoluÃ§Ã£o: Acesso Coalescido
```python
# âœ… Bom: threads consecutivas acessam posiÃ§Ãµes consecutivas
@cuda.jit
def good_memory_access(data):
    idx = cuda.grid(1)
    if idx < len(data):
        # Threads consecutivas acessam posiÃ§Ãµes adjacentes
        data[idx] = idx  # Stride = 1
```

---

## ğŸ”¥ AplicaÃ§Ã£o: SimulaÃ§Ã£o de DifusÃ£o de Calor

### Problema FÃ­sico

#### EquaÃ§Ã£o de DifusÃ£o 2D
```
âˆ‚T/âˆ‚t = Î± * (âˆ‚Â²T/âˆ‚xÂ² + âˆ‚Â²T/âˆ‚yÂ²)

Onde:
T = temperatura
Î± = difusividade tÃ©rmica
t = tempo
x, y = coordenadas espaciais
```

#### DiscretizaÃ§Ã£o (DiferenÃ§as Finitas)
```
T[i,j]^(n+1) = T[i,j]^n + Î±*dt/dxÂ² * (
    T[i+1,j]^n + T[i-1,j]^n + T[i,j+1]^n + T[i,j-1]^n - 4*T[i,j]^n
)
```

### ImplementaÃ§Ã£o CPU (Baseline)

```python
def heat_equation_cpu(T, alpha, dt, dx, steps):
    """VersÃ£o CPU para comparaÃ§Ã£o"""
    ny, nx = T.shape
    T_new = np.zeros_like(T)
    
    for step in range(steps):
        for i in range(1, ny-1):
            for j in range(1, nx-1):
                T_new[i,j] = T[i,j] + alpha * dt / dx**2 * (
                    T[i+1,j] + T[i-1,j] + T[i,j+1] + T[i,j-1] - 4*T[i,j]
                )
        
        # Swap arrays
        T, T_new = T_new, T
        
        # Boundary conditions (exemplo: bordas fixas em 0Â°C)
        T[0,:] = T[-1,:] = T[:,0] = T[:,-1] = 0
    
    return T
```

### ImplementaÃ§Ã£o GPU (CuPy)

```python
def heat_equation_cupy(T, alpha, dt, dx, steps):
    """VersÃ£o CuPy - vetorizada"""
    T = cp.asarray(T)
    T_new = cp.zeros_like(T)
    
    for step in range(steps):
        # OperaÃ§Ã£o vetorizada - todos os pontos simultaneamente
        T_new[1:-1, 1:-1] = T[1:-1, 1:-1] + alpha * dt / dx**2 * (
            T[2:, 1:-1] + T[:-2, 1:-1] + 
            T[1:-1, 2:] + T[1:-1, :-2] - 4*T[1:-1, 1:-1]
        )
        
        # Swap arrays
        T, T_new = T_new, T
        
        # Boundary conditions
        T[0,:] = T[-1,:] = T[:,0] = T[:,-1] = 0
    
    return cp.asnumpy(T)
```

### ImplementaÃ§Ã£o GPU (Numba CUDA)

```python
@cuda.jit
def heat_step_kernel(T_old, T_new, alpha, dt, dx):
    """Kernel para um passo temporal da equaÃ§Ã£o do calor"""
    i, j = cuda.grid(2)
    
    if 1 <= i < T_old.shape[0]-1 and 1 <= j < T_old.shape[1]-1:
        T_new[i,j] = T_old[i,j] + alpha * dt / dx**2 * (
            T_old[i+1,j] + T_old[i-1,j] + 
            T_old[i,j+1] + T_old[i,j-1] - 4*T_old[i,j]
        )

def heat_equation_cuda(T, alpha, dt, dx, steps):
    """VersÃ£o Numba CUDA - controle fino"""
    T = cp.asarray(T)
    T_new = cp.zeros_like(T)
    
    # ConfiguraÃ§Ã£o do grid 2D
    threads_per_block = (16, 16)
    blocks_per_grid_x = (T.shape[1] + threads_per_block[1] - 1) // threads_per_block[1]
    blocks_per_grid_y = (T.shape[0] + threads_per_block[0] - 1) // threads_per_block[0]
    blocks_per_grid = (blocks_per_grid_y, blocks_per_grid_x)
    
    for step in range(steps):
        heat_step_kernel[blocks_per_grid, threads_per_block](T, T_new, alpha, dt, dx)
        cuda.synchronize()  # Aguardar conclusÃ£o
        
        # Swap arrays
        T, T_new = T_new, T
        
        # Boundary conditions (pode ser feito em kernel separado)
        T[0,:] = T[-1,:] = T[:,0] = T[:,-1] = 0
    
    return cp.asnumpy(T)
```

---

## ğŸ“ˆ Performance Analysis: CPU vs GPU

### Benchmarking Estruturado

```python
def benchmark_heat_equation():
    # ParÃ¢metros da simulaÃ§Ã£o
    sizes = [128, 256, 512, 1024]
    steps = 1000
    alpha, dt, dx = 0.01, 0.001, 0.1
    
    results = []
    
    for size in sizes:
        # CondiÃ§Ã£o inicial
        T_init = np.zeros((size, size))
        T_init[size//4:3*size//4, size//4:3*size//4] = 100  # Fonte de calor central
        
        # CPU
        start = time.time()
        T_cpu = heat_equation_cpu(T_init.copy(), alpha, dt, dx, steps)
        time_cpu = time.time() - start
        
        # GPU (CuPy)
        start = time.time()
        T_cupy = heat_equation_cupy(T_init.copy(), alpha, dt, dx, steps)
        time_cupy = time.time() - start
        
        # GPU (Numba CUDA)
        start = time.time()
        T_cuda = heat_equation_cuda(T_init.copy(), alpha, dt, dx, steps)
        time_cuda = time.time() - start
        
        results.append({
            'size': size,
            'points': size**2,
            'time_cpu': time_cpu,
            'time_cupy': time_cupy,
            'time_cuda': time_cuda,
            'speedup_cupy': time_cpu / time_cupy,
            'speedup_cuda': time_cpu / time_cuda
        })
    
    return results
```

### Resultados TÃ­picos

| Grid Size | CPU (s) | CuPy (s) | CUDA (s) | Speedup CuPy | Speedup CUDA |
|-----------|---------|----------|----------|--------------|--------------|
| 128Ã—128   | 0.5     | 0.05     | 0.03     | 10x          | 17x          |
| 256Ã—256   | 2.1     | 0.12     | 0.08     | 18x          | 26x          |
| 512Ã—512   | 8.7     | 0.31     | 0.19     | 28x          | 46x          |
| 1024Ã—1024 | 35.2    | 0.89     | 0.51     | 40x          | 69x          |

![GPU Performance Comparison](figures/gpu_performance_comparison.png)

### AnÃ¡lise dos Resultados

#### Por que GPU Ã© Mais RÃ¡pida?

1. **Paralelismo massivo**: 1M pontos processados simultaneamente
2. **Bandwidth**: Acesso eficiente Ã  memÃ³ria de alta velocidade
3. **Specialized units**: Hardware otimizado para floating-point
4. **No overhead**: Sem criaÃ§Ã£o/destruiÃ§Ã£o de threads

#### Quando GPU NÃ£o Compensa?

1. **Problemas pequenos**: Overhead de transferÃªncia
2. **CÃ³digo complexo**: Muitas branches e condicionais
3. **I/O intensivo**: GPU nÃ£o ajuda com disco/rede
4. **Algoritmos sequenciais**: DependÃªncias entre operaÃ§Ãµes

---

## âš¡ OtimizaÃ§Ãµes AvanÃ§adas

### Memory Access Patterns

#### Optimal: Coalesced Access
```python
# âœ… Threads consecutivas acessam elementos consecutivos
@cuda.jit
def coalesced_access(data):
    idx = cuda.grid(1)
    if idx < len(data):
        data[idx] = math.sqrt(data[idx])  # Stride = 1
```

#### Suboptimal: Strided Access
```python
# âš ï¸ Threads consecutivas acessam elementos distantes
@cuda.jit
def strided_access(data):
    idx = cuda.grid(1)
    if idx < len(data) // 8:
        data[idx * 8] = math.sqrt(data[idx * 8])  # Stride = 8
```

### Shared Memory Utilization

```python
@cuda.jit
def reduction_sum(data, result):
    """Soma reduÃ§Ã£o usando shared memory"""
    # Shared memory por block
    shared = cuda.shared.array(256, numba.float32)
    
    tid = cuda.threadIdx.x
    bid = cuda.blockIdx.x
    idx = bid * cuda.blockDim.x + tid
    
    # Carregar dado para shared memory
    if idx < len(data):
        shared[tid] = data[idx]
    else:
        shared[tid] = 0.0
    
    cuda.syncthreads()
    
    # ReduÃ§Ã£o tree-based
    stride = 1
    while stride < cuda.blockDim.x:
        if tid % (2 * stride) == 0 and tid + stride < cuda.blockDim.x:
            shared[tid] += shared[tid + stride]
        stride *= 2
        cuda.syncthreads()
    
    # Thread 0 escreve resultado do block
    if tid == 0:
        cuda.atomic.add(result, 0, shared[0])
```

### Occupancy Optimization

```python
# Calcular occupancy para otimizar threads_per_block
def optimize_occupancy(kernel, max_threads=1024):
    best_occupancy = 0
    best_threads = 32
    
    for threads in range(32, max_threads + 1, 32):  # MÃºltiplos de 32
        try:
            occupancy = cuda.get_max_active_blocks_per_multiprocessor(
                kernel, threads, 0  # 0 = sem shared memory
            )
            if occupancy > best_occupancy:
                best_occupancy = occupancy
                best_threads = threads
        except:
            continue
    
    return best_threads, best_occupancy
```

---

## ğŸš¨ LimitaÃ§Ãµes e ConsideraÃ§Ãµes

### Hardware Requirements

#### GPU Memory
```python
# Verificar memÃ³ria disponÃ­vel
free_mem, total_mem = cp.cuda.runtime.memGetInfo()
print(f"GPU Memory: {free_mem // (1024**3)} GB free of {total_mem // (1024**3)} GB")

# Estimar uso para array
array_size = 1000 * 1000 * 8  # 1M floats * 8 bytes
if array_size > free_mem:
    print("âš ï¸ Array muito grande para GPU memory!")
```

#### Compute Capability
```python
# Verificar capacidades da GPU
device = cp.cuda.Device()
compute_capability = device.compute_capability
print(f"Compute Capability: {compute_capability}")

# Recursos disponÃ­veis por capability
if compute_capability >= (7, 0):  # Volta+
    print("âœ… Tensor Cores disponÃ­veis")
if compute_capability >= (6, 0):  # Pascal+
    print("âœ… Unified Memory suportado")
```

### Code Limitations

#### Numba CUDA Restrictions
```python
# âŒ NÃ£o funciona em kernels CUDA
@cuda.jit
def problematic_kernel():
    # RecursÃ£o
    return fibonacci(n)  # Erro!
    
    # Dynamic allocation
    arr = []  # Erro!
    
    # Print statements
    print("Debug")  # Erro!
    
    # Python objects
    my_dict = {}  # Erro!

# âœ… Alternativas
@cuda.jit
def good_kernel(data):
    # Loops simples
    for i in range(100):
        data[i] *= 2
    
    # Math functions
    result = math.sqrt(data[0])
    
    # Array operations
    data[0] = data[1] + data[2]
```

### Performance Pitfalls

#### Memory Transfer Overhead
```python
# âŒ TransferÃªncia excessiva
def bad_gpu_usage():
    for i in range(1000):
        data_gpu = cp.asarray(data_cpu)  # CPUâ†’GPU transfer
        result_gpu = cp.sqrt(data_gpu)
        result_cpu = cp.asnumpy(result_gpu)  # GPUâ†’CPU transfer

# âœ… Minimizar transferÃªncias
def good_gpu_usage():
    data_gpu = cp.asarray(data_cpu)  # Uma transferÃªncia
    for i in range(1000):
        data_gpu = cp.sqrt(data_gpu)  # Tudo na GPU
    result_cpu = cp.asnumpy(data_gpu)  # Uma transferÃªncia
```

#### Underutilization
```python
# âŒ Poucos threads
@cuda.jit
def underutilized_kernel(data):
    idx = cuda.grid(1)
    if idx == 0:  # Apenas thread 0 trabalha!
        for i in range(len(data)):
            data[i] *= 2

# âœ… Paralelismo adequado
@cuda.jit
def well_utilized_kernel(data):
    idx = cuda.grid(1)
    if idx < len(data):  # Todas as threads trabalham
        data[idx] *= 2
```

---

## ğŸ¯ Diretrizes de Uso

### Decision Tree: CPU vs GPU

```
Problema computacional
â”œâ”€â”€ Tamanho dos dados?
â”‚   â”œâ”€â”€ < 1MB â†’ CPU (overhead nÃ£o compensa)
â”‚   â””â”€â”€ > 1MB â†’ Continue
â”œâ”€â”€ Tipo de operaÃ§Ã£o?
â”‚   â”œâ”€â”€ Sequencial/Branch-heavy â†’ CPU
â”‚   â”œâ”€â”€ Data parallel â†’ Continue
â”‚   â””â”€â”€ I/O intensive â†’ CPU
â”œâ”€â”€ FrequÃªncia de execuÃ§Ã£o?
â”‚   â”œâ”€â”€ Uma vez â†’ CPU (setup overhead)
â”‚   â””â”€â”€ Repetitiva â†’ GPU
â””â”€â”€ Hardware disponÃ­vel?
    â”œâ”€â”€ Sem GPU â†’ CPU
    â””â”€â”€ Com GPU â†’ GPU!
```

### Workflow de Desenvolvimento

1. **Implementar CPU** version primeiro (baseline)
2. **Profile** e identificar gargalos
3. **Port para CuPy** (mudanÃ§a simples npâ†’cp)
4. **Medir speedup** e identificar limitaÃ§Ãµes
5. **Custom kernels** se necessÃ¡rio (Numba CUDA)
6. **Otimizar** memory access patterns
7. **Benchmark** final e documentar

### Tools e Debugging

```python
# Profiling GPU
cp.cuda.profiler.start()
# ... cÃ³digo GPU ...
cp.cuda.profiler.stop()

# Memory debugging
print(f"GPU memory used: {cp.get_default_memory_pool().used_bytes()}")

# Timing preciso
with cp.cuda.Device(0):
    start = cp.cuda.Event()
    end = cp.cuda.Event()
    start.record()
    # ... cÃ³digo GPU ...
    end.record()
    end.synchronize()
    elapsed = cp.cuda.get_elapsed_time(start, end)  # ms
```

---

## ğŸ¯ Pontos Principais para Recordar

### Arquitetura e Paradigmas
1. **GPU**: Throughput vs **CPU**: Latency
2. **SIMT model**: Milhares de threads simples
3. **Memory hierarchy**: Global vs Shared vs Registers
4. **Coalesced access**: PadrÃ£o de acesso crucial

### Ferramentas Python
1. **CuPy**: Drop-in replacement para NumPy
2. **Numba CUDA**: Kernels customizados
3. **Memory management**: TransferÃªncias sÃ£o caras
4. **Profiling**: Essencial para otimizaÃ§Ã£o

### AplicaÃ§Ãµes em Engenharia
1. **SimulaÃ§Ãµes numÃ©ricas**: DifusÃ£o, CFD, FEM
2. **Ãlgebra linear**: Sistemas grandes, eigenvalues
3. **Monte Carlo**: AnÃ¡lise probabilÃ­stica
4. **Image processing**: InspeÃ§Ã£o, monitoramento

![Course Evolution](figures/course_evolution.png)

---

## ğŸš€ PrÃ³ximos Passos

### Nesta Aula
- Experimentar com CuPy e Numba CUDA
- Implementar simulaÃ§Ã£o de difusÃ£o de calor
- Medir e comparar performance CPU vs GPU

### Projeto Final
- Aplicar tÃ©cnicas de todas as 3 aulas
- CPU multiprocessing + GPU computing
- AnÃ¡lise completa de escalabilidade
- AplicaÃ§Ã£o em problema real de engenharia

### Desenvolvimento Futuro
- **Deep Learning**: PyTorch, TensorFlow
- **Multi-GPU**: Scaling para mÃºltiplas GPUs
- **Cloud computing**: AWS, Google Cloud GPUs
- **Specialized libraries**: Thrust, cuBLAS, cuSPARSE

---

## â“ Perguntas para ReflexÃ£o

1. **Memory bound vs Compute bound**: Como identificar o gargalo?

2. **Quando GPU nÃ£o vale a pena?** Que sinais indicam isso?

3. **Algoritmos sequenciais**: Como adaptar para GPU?

4. **Debugging GPU code**: Que estratÃ©gias usar?

**Desafio**: Identifique um algoritmo lento do seu trabalho e avalie se GPU computing pode ajudar!

---

**Vamos para o Notebook! ğŸ’»**