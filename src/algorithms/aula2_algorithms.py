"""
Algoritmos para Aula 2 - Paralelismo AvanÃ§ado e Escalabilidade
FunÃ§Ãµes otimizadas para demonstrar speedup real em diferentes cenÃ¡rios
"""

import numpy as np
import time
import cProfile
import pstats
import io
from functools import wraps
from joblib import Parallel, delayed
import multiprocessing as mp

# =============================================================================
# Performance Measurement Tools
# =============================================================================

def time_function(func):
    """Decorator para medir tempo de execuÃ§Ã£o"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start = time.perf_counter()
        result = func(*args, **kwargs)
        end = time.perf_counter()
        print(f"{func.__name__}: {end - start:.4f}s")
        return result
    return wrapper

def profile_function(func, *args, **kwargs):
    """Profile detalhado de uma funÃ§Ã£o"""
    pr = cProfile.Profile()
    pr.enable()
    result = func(*args, **kwargs)
    pr.disable()
    
    s = io.StringIO()
    ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')
    ps.print_stats(10)  # Top 10 funÃ§Ãµes
    
    print("ğŸ“Š Profile detalhado:")
    print(s.getvalue())
    return result

@time_function
def compute_intensive_task(n):
    """Tarefa computacionalmente intensiva para testes"""
    total = 0
    for i in range(n):
        total += i ** 2
    return total

# =============================================================================
# Statistical Analysis Algorithms (Intensive)
# =============================================================================

def linear_regression_simple(data_chunk):
    """
    VersÃ£o SIMPLES - demonstra overhead dominando
    """
    X, y = data_chunk
    
    # RegressÃ£o bÃ¡sica rÃ¡pida
    XtX = np.dot(X.T, X)
    Xty = np.dot(X.T, y)
    beta = np.linalg.solve(XtX, Xty)
    
    # RÂ² bÃ¡sico
    y_pred = np.dot(X, beta)
    ss_res = np.sum((y - y_pred) ** 2)
    ss_tot = np.sum((y - np.mean(y)) ** 2)
    r_squared = 1 - (ss_res / ss_tot)
    
    return {'beta': beta, 'r_squared': r_squared}

def linear_regression_intensive_light(data_chunk):
    """
    Worker INTENSIVO LEVE para regressÃ£o linear - versÃ£o demo mais rÃ¡pida
    Bootstrap com apenas 100 amostras para demonstraÃ§Ã£o
    """
    X, y = data_chunk
    
    # RegressÃ£o bÃ¡sica
    XtX = np.dot(X.T, X)
    Xty = np.dot(X.T, y)
    beta = np.linalg.solve(XtX, Xty)
    
    n, p = X.shape
    
    # BOOTSTRAP LEVE - 100 amostras (vs 500 na versÃ£o completa)
    bootstrap_betas = []
    n_bootstrap = 100  # Reduzido para demo mais rÃ¡pida
    
    np.random.seed(42)  # Reproducibilidade
    for i in range(n_bootstrap):
        # Amostra bootstrap
        indices = np.random.choice(n, size=n, replace=True)
        X_boot = X[indices]
        y_boot = y[indices]
        
        # RegressÃ£o na amostra bootstrap
        XtX_boot = np.dot(X_boot.T, X_boot)
        Xty_boot = np.dot(X_boot.T, y_boot)
        
        # RegularizaÃ§Ã£o para estabilidade
        ridge_param = 1e-6
        XtX_boot += ridge_param * np.eye(p)
        
        try:
            beta_boot = np.linalg.solve(XtX_boot, Xty_boot)
            bootstrap_betas.append(beta_boot)
        except np.linalg.LinAlgError:
            continue
    
    bootstrap_betas = np.array(bootstrap_betas)
    
    # Intervalos de confianÃ§a
    confidence_intervals = []
    if len(bootstrap_betas) > 0:
        for j in range(p):
            ci_lower = np.percentile(bootstrap_betas[:, j], 2.5)
            ci_upper = np.percentile(bootstrap_betas[:, j], 97.5)
            confidence_intervals.append((ci_lower, ci_upper))
    
    # Calcular RÂ² e estatÃ­sticas
    y_pred = np.dot(X, beta)
    ss_res = np.sum((y - y_pred) ** 2)
    ss_tot = np.sum((y - np.mean(y)) ** 2)
    r_squared = 1 - (ss_res / ss_tot)
    
    return {
        'beta': beta,
        'r_squared': r_squared,
        'confidence_intervals': confidence_intervals,
        'n_bootstrap': len(bootstrap_betas)
    }


def linear_regression_intensive(data_chunk):
    """
    VersÃ£o INTENSIVA - demonstra speedup real
    AnÃ¡lise estatÃ­stica completa com:
    - Bootstrap para intervalos de confianÃ§a
    - AnÃ¡lise de resÃ­duos
    - EstatÃ­sticas avanÃ§adas
    """
    X, y = data_chunk
    
    # RegressÃ£o bÃ¡sica
    XtX = np.dot(X.T, X)
    Xty = np.dot(X.T, y)
    beta = np.linalg.solve(XtX, Xty)
    
    n, p = X.shape
    
    # BOOTSTRAP INTENSIVO - 500 amostras
    bootstrap_betas = []
    n_bootstrap = 500
    
    np.random.seed(42)  # Reproducibilidade
    for i in range(n_bootstrap):
        # Amostra bootstrap
        indices = np.random.choice(n, size=n, replace=True)
        X_boot = X[indices]
        y_boot = y[indices]
        
        # RegressÃ£o na amostra bootstrap
        XtX_boot = np.dot(X_boot.T, X_boot)
        Xty_boot = np.dot(X_boot.T, y_boot)
        
        # RegularizaÃ§Ã£o para estabilidade
        ridge_param = 1e-6
        XtX_boot += ridge_param * np.eye(p)
        
        try:
            beta_boot = np.linalg.solve(XtX_boot, Xty_boot)
            bootstrap_betas.append(beta_boot)
        except np.linalg.LinAlgError:
            continue
    
    bootstrap_betas = np.array(bootstrap_betas)
    
    # INTERVALOS DE CONFIANÃ‡A
    confidence_intervals = []
    if len(bootstrap_betas) > 0:
        for j in range(p):
            ci_lower = np.percentile(bootstrap_betas[:, j], 2.5)
            ci_upper = np.percentile(bootstrap_betas[:, j], 97.5)
            confidence_intervals.append((ci_lower, ci_upper))
    
    # ANÃLISE DE RESÃDUOS
    y_pred = np.dot(X, beta)
    ss_res = np.sum((y - y_pred) ** 2)
    ss_tot = np.sum((y - np.mean(y)) ** 2)
    r_squared = 1 - (ss_res / ss_tot)
    
    residuals = y - y_pred
    residual_stats = {
        'mean': np.mean(residuals),
        'std': np.std(residuals),
        'skewness': np.mean(((residuals - np.mean(residuals)) / np.std(residuals))**3),
        'kurtosis': np.mean(((residuals - np.mean(residuals)) / np.std(residuals))**4)
    }
    
    return {
        'beta': beta,
        'r_squared': r_squared,
        'confidence_intervals': confidence_intervals,
        'residual_stats': residual_stats,
        'n_bootstrap': len(bootstrap_betas)
    }

def run_regression_analysis_comparison(n_datasets=100, n_points=500, n_features=5, simple=False, light_mode=None):
    """
    Compara regressÃ£o serial vs paralela
    light_mode: None (auto), True (light), False (full)
    """
    # Gerar datasets
    datasets = []
    for i in range(n_datasets):
        X = np.random.randn(n_points, n_features)
        X = np.column_stack([np.ones(n_points), X])  # Intercept
        true_beta = np.random.randn(n_features + 1)
        y = np.dot(X, true_beta) + 0.1 * np.random.randn(n_points)
        datasets.append((X, y))
    
    # Escolher funÃ§Ã£o baseada nos parÃ¢metros
    if simple:
        func = linear_regression_simple
        analysis_type = "SIMPLES"
    else:
        # Auto-detectar se deve usar versÃ£o light
        if light_mode is None:
            # Use light mode se dataset Ã© pequeno (< 50 datasets ou < 400 pontos)
            use_light = n_datasets < 50 or n_points < 400
        else:
            use_light = light_mode
            
        if use_light:
            func = linear_regression_intensive_light
            analysis_type = "INTENSIVA (Bootstrap 100 amostras - DEMO)"
        else:
            func = linear_regression_intensive
            analysis_type = "INTENSIVA (Bootstrap 500 amostras - COMPLETA)"
    
    print(f"ğŸ§® RegressÃ£o {analysis_type}")
    print(f"â€¢ {n_datasets} datasets: {n_points} pontos, {n_features} features")
    
    if not simple:
        bootstrap_count = 100 if 'DEMO' in analysis_type else 500
        print(f"â€¢ Bootstrap: {bootstrap_count} amostras por regressÃ£o")
        print("â€¢ Intervalos de confianÃ§a e anÃ¡lise de resÃ­duos")
    
    # Serial
    start_time = time.perf_counter()
    results_serial = [func(data) for data in datasets]
    time_serial = time.perf_counter() - start_time
    
    # Paralelo
    start_time = time.perf_counter()
    results_parallel = Parallel(n_jobs=mp.cpu_count())(
        delayed(func)(data) for data in datasets
    )
    time_parallel = time.perf_counter() - start_time
    
    # MÃ©tricas
    speedup = time_serial / time_parallel
    efficiency = speedup / mp.cpu_count()
    
    return {
        'analysis_type': analysis_type,
        'n_datasets': n_datasets,
        'time_serial': time_serial,
        'time_parallel': time_parallel,
        'speedup': speedup,
        'efficiency': efficiency,
        'results': results_parallel
    }

# =============================================================================
# Numba Functions (if available)
# =============================================================================

def get_numba_functions():
    """
    Retorna funÃ§Ãµes Numba se disponÃ­vel
    """
    try:
        from numba import jit, prange
        
        def matrix_mult_python(A, B):
            """MultiplicaÃ§Ã£o em Python puro"""
            rows_A, cols_A = A.shape
            rows_B, cols_B = B.shape
            C = np.zeros((rows_A, cols_B))
            for i in range(rows_A):
                for j in range(cols_B):
                    for k in range(cols_A):
                        C[i, j] += A[i, k] * B[k, j]
            return C
        
        @jit(nopython=True)
        def matrix_mult_numba_serial(A, B):
            """MultiplicaÃ§Ã£o com Numba serial"""
            rows_A, cols_A = A.shape
            rows_B, cols_B = B.shape
            C = np.zeros((rows_A, cols_B))
            for i in range(rows_A):
                for j in range(cols_B):
                    for k in range(cols_A):
                        C[i, j] += A[i, k] * B[k, j]
            return C
        
        @jit(nopython=True, parallel=True)
        def matrix_mult_numba_parallel(A, B):
            """MultiplicaÃ§Ã£o com Numba paralelo"""
            rows_A, cols_A = A.shape
            rows_B, cols_B = B.shape
            C = np.zeros((rows_A, cols_B))
            for i in prange(rows_A):
                for j in range(cols_B):
                    for k in range(cols_A):
                        C[i, j] += A[i, k] * B[k, j]
            return C
        
        @jit(nopython=True)
        def complex_calculation_serial(n):
            """CÃ¡lculo complexo serial"""
            result = 0.0
            for i in range(n):
                result += np.sin(i) * np.cos(i) + np.sqrt(i + 1)
            return result
        
        @jit(nopython=True, parallel=True)
        def complex_calculation_parallel(n):
            """CÃ¡lculo complexo paralelo"""
            result = 0.0
            for i in prange(n):
                result += np.sin(i) * np.cos(i) + np.sqrt(i + 1)
            return result
        
        return {
            'available': True,
            'matrix_mult_python': matrix_mult_python,
            'matrix_mult_numba_serial': matrix_mult_numba_serial,
            'matrix_mult_numba_parallel': matrix_mult_numba_parallel,
            'complex_calculation_serial': complex_calculation_serial,
            'complex_calculation_parallel': complex_calculation_parallel
        }
        
    except ImportError:
        return {'available': False}

# =============================================================================
# Utility Functions
# =============================================================================

def format_speedup_result(speedup, efficiency=None):
    """Formata resultado de speedup de forma educativa"""
    if speedup > 1.0:
        improvement = (speedup - 1) * 100
        result = f"âœ… Speedup: {speedup:.2f}x ({improvement:.0f}% mais rÃ¡pido!)"
    else:
        degradation = (1 - speedup) * 100
        result = f"âš ï¸ Speedup: {speedup:.2f}x ({degradation:.0f}% mais lento - overhead domina)"
    
    if efficiency is not None:
        result += f"\n   EficiÃªncia: {efficiency:.2f} ({efficiency*100:.0f}%)"
    
    return result

def explain_overhead_vs_computation():
    """Explica o conceito de overhead vs computaÃ§Ã£o"""
    return """
ğŸ’¡ CONCEITO CHAVE: Overhead vs ComputaÃ§Ã£o

ğŸ”¹ OVERHEAD de multiprocessing:
  â€¢ CriaÃ§Ã£o e destruiÃ§Ã£o de processos
  â€¢ ComunicaÃ§Ã£o entre processos (IPC)
  â€¢ SerializaÃ§Ã£o/deserializaÃ§Ã£o de dados
  â€¢ SincronizaÃ§Ã£o e coordenaÃ§Ã£o

ğŸ”¹ COMPUTAÃ‡ÃƒO Ãºtil:
  â€¢ Algoritmos matemÃ¡ticos intensivos
  â€¢ Loops com muitas iteraÃ§Ãµes
  â€¢ OperaÃ§Ãµes matriciais complexas
  â€¢ AnÃ¡lises estatÃ­sticas pesadas

ğŸ¯ REGRA DE OURO:
  Speedup positivo APENAS quando: ComputaÃ§Ã£o >> Overhead

ğŸ“Š EXEMPLOS:
  â€¢ CÃ¡lculo simples: overhead domina â†’ speedup negativo
  â€¢ Bootstrap 500x: computaÃ§Ã£o domina â†’ speedup positivo
  â€¢ AnÃ¡lise modal: computaÃ§Ã£o domina â†’ speedup positivo
"""